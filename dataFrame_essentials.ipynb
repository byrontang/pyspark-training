{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import countDistinct, avg, stddev\n",
    "from pyspark.sql.functions import dayofmonth, dayofyear, year, month, hour, weekofyear, date_format\n",
    "from pyspark.sql.functions import col as func_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = 'dataFrame'\n",
    "spark = SparkSession.builder.appName(app_name).getOrCreate() # singleton instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = '..\\\\pyspark-training\\\\data\\\\Employees.json'\n",
    "csv_file_path = '..\\pyspark-training\\data\\AppleStore.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = spark.read.json(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| ID|   Name|Salary|\n",
      "+---+-------+------+\n",
      "|  1|   John| 20000|\n",
      "|  2|  Rohit| 15000|\n",
      "|  3|  Parth| 14600|\n",
      "|  4|Rishabh| 20500|\n",
      "|  5|  Daisy| 34000|\n",
      "+---+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=1, Name='John', Salary=20000),\n",
       " Row(ID=2, Name='Rohit', Salary=15000),\n",
       " Row(ID=3, Name='Parth', Salary=14600),\n",
       " Row(ID=4, Name='Rishabh', Salary=20500),\n",
       " Row(ID=5, Name='Daisy', Salary=34000)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# head() will return a list of Row objects\n",
    "json_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID', 'Name', 'Salary']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, ID: string, Salary: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+------------------+\n",
      "|summary|              ID|            Salary|\n",
      "+-------+----------------+------------------+\n",
      "|  count|              15|                15|\n",
      "|   mean|             8.0|           25940.0|\n",
      "| stddev|4.47213595499958|15075.183012393012|\n",
      "|    min|               1|             14600|\n",
      "|    max|              15|             70000|\n",
      "+-------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'Salary'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df['Salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json_df['Salary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike python DataFrame, pyspark DataFrame requires select() function to actually get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Salary|\n",
      "+------+\n",
      "| 20000|\n",
      "| 15000|\n",
      "| 14600|\n",
      "| 20500|\n",
      "| 34000|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.select(['Salary']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| ID|Salary|\n",
      "+---+------+\n",
      "|  1| 20000|\n",
      "|  2| 15000|\n",
      "|  3| 14600|\n",
      "|  4| 20500|\n",
      "|  5| 34000|\n",
      "+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.select(['ID', 'Salary']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|new_id|Base Salary|\n",
      "+------+-----------+\n",
      "|  1001|      20000|\n",
      "|  1002|      15000|\n",
      "|  1003|      14600|\n",
      "+------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# asias column names\n",
    "json_df.select((json_df.ID + 1000).alias('new_id'), json_df.Salary.alias('Base Salary')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new column\n",
    "A different approach than Python DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------+\n",
      "| ID| Name|Salary| Bonus|\n",
      "+---+-----+------+------+\n",
      "|  1| John| 20000|1000.0|\n",
      "|  2|Rohit| 15000| 750.0|\n",
      "|  3|Parth| 14600| 730.0|\n",
      "+---+-----+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.withColumn('Bonus', json_df['Salary'] * 0.05).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You may execute withColumn multiple times as it just overwrites the same column name everytime without any error. ALso, the changes made here are not permanent to DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'Name', 'Salary']\n"
     ]
    }
   ],
   "source": [
    "print(json_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+\n",
      "| ID| Name|New Salary|\n",
      "+---+-----+----------+\n",
      "|  1| John|     20000|\n",
      "|  2|Rohit|     15000|\n",
      "|  3|Parth|     14600|\n",
      "+---+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.withColumnRenamed('Salary', 'New Salary').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SQL\n",
    "To use SQL queries over dataframe, you need to register it as a temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| ID| Name|Salary|\n",
      "+---+-----+------+\n",
      "|  1| John| 20000|\n",
      "|  2|Rohit| 15000|\n",
      "|  3|Parth| 14600|\n",
      "+---+-----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.createOrReplaceTempView('Emp')\n",
    "sql_df = spark.sql('select * from Emp')\n",
    "sql_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|salary| Bonus|\n",
      "+------+------+\n",
      "| 15000|750.00|\n",
      "| 14600|730.00|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('select salary, salary * 0.05 as Bonus from Emp where Salary <= 15000')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- size_bytes: long (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- rating_count_tot: integer (nullable = true)\n",
      " |-- rating_count_ver: integer (nullable = true)\n",
      " |-- user_rating: double (nullable = true)\n",
      " |-- user_rating_ver: double (nullable = true)\n",
      " |-- ver: string (nullable = true)\n",
      " |-- cont_rating: string (nullable = true)\n",
      " |-- prime_genre: string (nullable = true)\n",
      " |-- sup_devices.num: integer (nullable = true)\n",
      " |-- ipadSc_urls.num: integer (nullable = true)\n",
      " |-- lang.num: integer (nullable = true)\n",
      " |-- vpp_lic: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let Spark know about the header and infer the schema types.\n",
    "# This is only available as option on CSVs and not on JSON files.\n",
    "appStore_df = spark.read.csv(csv_file_path, inferSchema=True, header=True)\n",
    "appStore_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=1, id=281656475, track_name='PAC-MAN Premium', size_bytes=100788224, currency='USD', price=3.99, rating_count_tot=21292, rating_count_ver=26, user_rating=4.0, user_rating_ver=4.5, ver='6.3.5', cont_rating='4+', prime_genre='Games', sup_devices.num=38, ipadSc_urls.num=5, lang.num=10, vpp_lic=1),\n",
       " Row(_c0=2, id=281796108, track_name='Evernote - stay organized', size_bytes=158578688, currency='USD', price=0.0, rating_count_tot=161065, rating_count_ver=26, user_rating=4.0, user_rating_ver=3.5, ver='8.2.2', cont_rating='4+', prime_genre='Productivity', sup_devices.num=37, ipadSc_urls.num=5, lang.num=23, vpp_lic=1),\n",
       " Row(_c0=3, id=281940292, track_name='WeatherBug - Local Weather, Radar, Maps, Alerts', size_bytes=100524032, currency='USD', price=0.0, rating_count_tot=188583, rating_count_ver=2822, user_rating=3.5, user_rating_ver=4.5, ver='5.0.0', cont_rating='4+', prime_genre='Weather', sup_devices.num=37, ipadSc_urls.num=5, lang.num=3, vpp_lic=1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appStore_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----+-----------+\n",
      "|       id|          track_name|price|user_rating|\n",
      "+---------+--------------------+-----+-----------+\n",
      "|281656475|     PAC-MAN Premium| 3.99|        4.0|\n",
      "|281796108|Evernote - stay o...|  0.0|        4.0|\n",
      "|281940292|WeatherBug - Loca...|  0.0|        3.5|\n",
      "+---------+--------------------+-----+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appStore_df.select(['id', 'track_name', 'price', 'user_rating']).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7197"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appStore_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|          track_name| price|\n",
      "+--------------------+------+\n",
      "|iReal Pro - Music...| 12.99|\n",
      "|                 大辞林| 21.99|\n",
      "|Proloquo2Go - Sym...|249.99|\n",
      "|      NAVIGON Europe| 74.99|\n",
      "|    Gaia GPS Classic| 19.99|\n",
      "|AnkiMobile Flashc...| 24.99|\n",
      "|Pocket Anatomy - ...| 13.99|\n",
      "|iStatVball 2 iPad...| 19.99|\n",
      "|   FINAL FANTASY III| 14.99|\n",
      "|      Site Audit Pro| 16.99|\n",
      "|FINAL FANTASY III...| 16.99|\n",
      "|プチ・ロワイヤル仏和辞典（第4版）...| 47.99|\n",
      "|    FL Studio Mobile| 13.99|\n",
      "|          FiLMiC Pro| 14.99|\n",
      "|Human Anatomy Atl...| 24.99|\n",
      "|FINAL FANTASY TAC...| 13.99|\n",
      "|         STEINS;GATE| 24.99|\n",
      "|              Notion| 14.99|\n",
      "|iDoceo - teacher'...| 11.99|\n",
      "|Articulation Stat...| 59.99|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using SQL with select\n",
    "appStore_df.filter('price > 9.99').select(['track_name', 'price']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex filtering using Python operators for comparison\n",
    "Syntaxlooks very similar to SQL operators, excep we need to ensure that we call the entire column with the dataframe, using the format: df['column name']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|       track_name|price|\n",
      "+-----------------+-----+\n",
      "|   ウィズダム英和・和英辞典 2|23.99|\n",
      "|         Model 15|29.99|\n",
      "| SkySafari 5 Plus|14.99|\n",
      "|AUM - Audio Mixer|18.99|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appStore_df.filter((appStore_df['price'] > 9.99) & (appStore_df['user_rating'] == 5)).select(['track_name', 'price']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Operators\n",
    "- | => or\n",
    "- & => and\n",
    "- \\- => not (equivalent to ! in Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result after filtering can be saved using collect(). It will be saved as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_tot_ratings = appStore_df.filter('rating_count_tot > 100000').select(['track_name', 'rating_count_tot']).collect()\n",
    "type(high_tot_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows can be converted to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rating_count_tot': 161065, 'track_name': 'Evernote - stay organized'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = high_tot_ratings[0]\n",
    "row.asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evernote - stay organized\n",
      "161065\n"
     ]
    }
   ],
   "source": [
    "# get all values:\n",
    "for item in row:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy, Agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "groupBy() returns a GroupedData object, of which various methods can be invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x20d5cf4bd68>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appStore_df.groupBy('prime_genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+---------------------+\n",
      "|  prime_genre|  avg(user_rating)|avg(rating_count_tot)|\n",
      "+-------------+------------------+---------------------+\n",
      "|    Education| 3.376379690949227|   2239.2295805739514|\n",
      "|   Navigation|2.6847826086956523|    11853.95652173913|\n",
      "|Entertainment|3.2467289719626167|    7533.678504672897|\n",
      "|       Sports| 2.982456140350877|   14026.929824561403|\n",
      "| Food & Drink|3.1825396825396823|   13938.619047619048|\n",
      "+-------------+------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appStore_df.groupBy('prime_genre').mean('user_rating', 'rating_count_tot').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use alias, we use **functions.col** imported as func_col above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|  prime_genre|  avg(user_rating)|\n",
      "+-------------+------------------+\n",
      "|    Education| 3.376379690949227|\n",
      "|   Navigation|2.6847826086956523|\n",
      "|Entertainment|3.2467289719626167|\n",
      "|       Sports| 2.982456140350877|\n",
      "| Food & Drink|3.1825396825396823|\n",
      "+-------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appStore_df.groupBy('prime_genre').mean('user_rating').alias('avg rating').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|  prime_genre|        avg_rating|\n",
      "+-------------+------------------+\n",
      "|    Education| 3.376379690949227|\n",
      "|   Navigation|2.6847826086956523|\n",
      "|Entertainment|3.2467289719626167|\n",
      "|       Sports| 2.982456140350877|\n",
      "| Food & Drink|3.1825396825396823|\n",
      "+-------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appStore_df.groupBy('prime_genre').mean('user_rating').select('prime_genre', func_col('avg(user_rating)').alias('avg_rating')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, **withColumnRenamed** can be used to rename an aggregate measure and generate alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|  prime_genre|        avg_rating|\n",
      "+-------------+------------------+\n",
      "|    Education| 3.376379690949227|\n",
      "|   Navigation|2.6847826086956523|\n",
      "|Entertainment|3.2467289719626167|\n",
      "+-------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appStore_df.groupBy('prime_genre').mean('user_rating').withColumnRenamed('avg(user_rating)', 'avg_rating').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using agg() to apply different functions across columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+------------------+\n",
      "|      prime_genre|max(price)|  avg(user_rating)|\n",
      "+-----------------+----------+------------------+\n",
      "|        Education|    299.99| 3.376379690949227|\n",
      "|       Navigation|     74.99|2.6847826086956523|\n",
      "|    Entertainment|      9.99|3.2467289719626167|\n",
      "|           Sports|     19.99| 2.982456140350877|\n",
      "|     Food & Drink|     27.99|3.1825396825396823|\n",
      "|    Photo & Video|     22.99|3.8008595988538683|\n",
      "|           Travel|      9.99| 3.376543209876543|\n",
      "|          Finance|      5.99|2.4326923076923075|\n",
      "|Social Networking|      9.99|2.9850299401197606|\n",
      "|             Book|     27.99|2.4776785714285716|\n",
      "|         Shopping|      1.99| 3.540983606557377|\n",
      "|        Reference|     47.99|          3.453125|\n",
      "| Health & Fitness|      9.99|               3.7|\n",
      "|        Utilities|     24.99| 3.278225806451613|\n",
      "|     Productivity|     99.99|  4.00561797752809|\n",
      "|            Games|     29.99|3.6850077679958573|\n",
      "|            Music|     49.99|3.9782608695652173|\n",
      "|        Lifestyle|      4.99|2.8055555555555554|\n",
      "|         Business|     59.99| 3.745614035087719|\n",
      "|         Catalogs|      7.99|               2.1|\n",
      "+-----------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appStore_df.groupBy('prime_genre').agg({'user_rating':'mean', 'price':'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple aggregations using **agg** on the same column along with **alias** on aggregate measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+------------+---------------+\n",
      "|  prime_genre|highest_price|lowest_price|distinct_prices|\n",
      "+-------------+-------------+------------+---------------+\n",
      "|    Education|       299.99|         0.0|             19|\n",
      "|   Navigation|        74.99|         0.0|             10|\n",
      "|Entertainment|         9.99|         0.0|              9|\n",
      "|       Sports|        19.99|         0.0|              9|\n",
      "| Food & Drink|        27.99|         0.0|             10|\n",
      "+-------------+-------------+------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max as smax, min as smin, count, countDistinct\n",
    "appStore_df.groupBy('prime_genre').agg(smax('price').alias('highest_price'),\n",
    "                                       smin('price').alias('lowest_price'),\n",
    "                                       countDistinct('price').alias('distinct_prices')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply aggregation functions directly on GroupedData without using \"agg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+------------------+\n",
      "|  prime_genre|        avg(price)|  avg(user_rating)|\n",
      "+-------------+------------------+------------------+\n",
      "|    Education| 4.028233995584995| 3.376379690949227|\n",
      "|   Navigation|4.1247826086956545|2.6847826086956523|\n",
      "|Entertainment|0.8897009345794415|3.2467289719626167|\n",
      "|       Sports|0.9530701754385958| 2.982456140350877|\n",
      "| Food & Drink|1.5523809523809518|3.1825396825396823|\n",
      "+-------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appStore_df.groupBy('prime_genre').mean('price', 'user_rating').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL Functions\n",
    "Various of functions that we can import from pyspark.sql.functions.\n",
    "\n",
    "```python\n",
    "df.select(dayofmonth(df['Date'])).show()\n",
    "```\n",
    "Also: \n",
    "```python\n",
    "monthofyear(), year(), countDistinct(), avg(), stddev()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "| ID|    Name|Salary|\n",
      "+---+--------+------+\n",
      "|  3|   Parth| 14600|\n",
      "|  2|   Rohit| 15000|\n",
      "| 12|    Riya| 17000|\n",
      "| 13|   Krish| 17000|\n",
      "| 14|Akanksha| 20000|\n",
      "+---+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Asending\n",
    "json_df.orderBy('Salary').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n",
      "| ID|    Name|Salary|\n",
      "+---+--------+------+\n",
      "|  9|   Varun| 70000|\n",
      "|  7|Sushmita| 50000|\n",
      "|  5|   Daisy| 34000|\n",
      "| 11| Johnson| 25500|\n",
      "|  6|   Annie| 23000|\n",
      "+---+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Desending\n",
    "json_df.orderBy(json_df['Salary'].desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "# To register UDF, we need a SQLContext\n",
    "# SQLContext is derived from a sparkContext\n",
    "sc = spark.sparkContext\n",
    "sql_ctx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_ctx.registerFunction('uname', lambda val: val.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql('select uname(Name), salary from Emp').show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested = spark.read.json('..\\\\pyspark-training\\\\data\\\\nested.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- phones: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- reporting: struct (nullable = true)\n",
      " |    |-- manager: string (nullable = true)\n",
      " |    |-- rm: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nested.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|  col| id|\n",
      "+-----+---+\n",
      "|12345|  1|\n",
      "|56789|  1|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phones = nested.select([explode('phones'), 'id'])\n",
    "phones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---+\n",
      "|manager|   rm| id|\n",
      "+-------+-----+---+\n",
      "|  Steve|Brian|  1|\n",
      "+-------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = nested.select(nested['reporting'].alias('tmp'), nested['id']).select(['tmp.*', 'id'])\n",
    "report.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Schema Management\n",
    "Shema helps to convert RDD to DataFrame.\n",
    "## Inferring the Schma Using Reflection\n",
    "A schema is the layout of the dataset in the form of attributes in the dataset and the datatype associated to that attribute. A schema provides the flexibility to switch between RDD and data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and convert each line to a Row to supply schema to RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = 'RDD Hands-on'\n",
    "conf = SparkConf().setAppName(app_name)\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Amy', '32'],\n",
       " ['Roger', '36'],\n",
       " ['Jake', '27'],\n",
       " ['Brian', '21'],\n",
       " ['Jen', '30']]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile('..\\\\pyspark-training\\\\data\\\\rdd_sample.csv')\n",
    "parts = lines.map(lambda l: l.split(','))\n",
    "parts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=32, name='Amy'),\n",
       " Row(age=36, name='Roger'),\n",
       " Row(age=27, name='Jake'),\n",
       " Row(age=21, name='Brian'),\n",
       " Row(age=30, name='Jen')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "people.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infer schema and register the DataFrame as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 32|  Amy|\n",
      "| 36|Roger|\n",
      "| 27| Jake|\n",
      "| 21|Brian|\n",
      "| 30|  Jen|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 32|  Amy|\n",
      "| 36|Roger|\n",
      "| 27| Jake|\n",
      "| 21|Brian|\n",
      "| 30|  Jen|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemaPeople.createOrReplaceTempView('people')\n",
    "spark.sql('select * from people').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatically Specifying the Schema\n",
    "Convert each line to a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Amy', '32'),\n",
       " ('Roger', '36'),\n",
       " ('Jake', '27'),\n",
       " ('Brian', '21'),\n",
       " ('Jen', '30')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = parts.map(lambda p: (p[0], p[1].strip()))\n",
    "people.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify schema encoded in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaString = 'name age'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Struct each field as string type; different types can be defined specifically to each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the tuple and apply the shema on the RDD to create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|  Amy| 32|\n",
      "|Roger| 36|\n",
      "| Jake| 27|\n",
      "|Brian| 21|\n",
      "|  Jen| 30|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemaPeople = sqlContext.createDataFrame(people, schema)\n",
    "schemaPeople.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Inferring Schema | Specifying Schema |\n",
    "| --- | --- |\n",
    "| Convert RDD to list of **Rows** and **infer schema** | Convert RDD to list of **tuples** |\n",
    "| Need not prepare schema | **Define schema string** and prepare **StructType** |\n",
    "| Call **createDataFrame** without schema | call **createDataFrame** with schema |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Metastore\n",
    "Spark SQL uses a Hive metastore to manage the metadata of persistent relational entities (e.g. databases, tables, columns, partitions) in a relational database (for fast access).\n",
    "\n",
    "A Hive metastore warehouse (aka spark-warehouse) is the directory where Spark SQL persists tables whereas a Hive metastore (aka metastore_db) is a relational database to manage the metadata of the persistent relational entities, e.g. databases, tables, columns, partitions.\n",
    "\n",
    "By default, Spark SQl uses the embedded deployment mode of a Hive metastore with a Apache Derbe database.\n",
    "\n",
    "When SparkSession is created with Hive support the external catalog (aka metasotre) is HiveExternalCatalog. HiveExternalCatalog uses spark.sql.warehouse.dir directory for the location of the databases.\n",
    "\n",
    "The benefits of using an external Hive metastore:\n",
    "- Allow a multiple Spark applications (sessions) to access it concurrently\n",
    "- Allow a single Spark application to use label statistics without running \"ANNALYZE TABLE\" every execution\n",
    "\n",
    "***Spark SQL uses the Hive-specific configuration properties that further fine-tune the Hive integration, e.g. spark.sql.hive.metastore.version or spark.sql.hive.metastore.jars.***\n",
    "\n",
    "**spark.sql.warehouse.dir** is a static configuration property that sets Hive's hive.metastore.warehouse.dir property, i.e. the locaitonof the Hive local/embedded metastore database (using Derby)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
