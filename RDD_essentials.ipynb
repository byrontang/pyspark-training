{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "from operator import add, mul\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\byron\\\\Documents\\\\GitHub\\\\pyspark-training'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd=os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a SparkContext object and name it.\n",
    "\n",
    "Use SparkContext.getOrCreate() to avoid error:\n",
    "https://stackoverflow.com/questions/46351951/valueerror-cannot-run-multiple-sparkcontexts-at-once-in-spark-with-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = 'RDD Hands-on'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(app_name)\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local-1576430619577'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.applicationId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile_path = ('..\\pyspark-training\\data\\pyspark_classes.txt')\n",
    "appleStore_path = ('..\\pyspark-training\\data\\AppleStore.csv')\n",
    "DATA_STR = 'PySpark is the Python API for Spark.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RDDs\n",
    "* Parallelized Collection\n",
    "\n",
    "A RDD can be created using a SparkContext object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PySpark', 'is', 'the', 'Python', 'API', 'for', 'Spark.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = DATA_STR.split(' ')\n",
    "pcoll = sc.parallelize(data)\n",
    "# collect() is only good for small dataset\n",
    "pcoll.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From another RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PYSPARK', 'IS', 'THE', 'PYTHON', 'API', 'FOR', 'SPARK.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_from_rdd = pcoll.map(lambda word: word.upper())\n",
    "rdd_from_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From external data - n the form of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Public classes:',\n",
       " 'SparkContext:',\n",
       " 'Main entry point for Spark functionality.',\n",
       " 'RDD:',\n",
       " 'A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.',\n",
       " 'Broadcast:',\n",
       " 'A broadcast variable that gets reused across tasks.',\n",
       " 'Accumulator:',\n",
       " 'An “add-only” shared variable that tasks can only add values to.',\n",
       " 'SparkConf:',\n",
       " 'For configuring Spark.',\n",
       " 'SparkFiles:',\n",
       " 'Access files shipped with jobs.',\n",
       " 'StorageLevel:',\n",
       " 'Finer-grained cache persistence levels.',\n",
       " 'TaskContext:',\n",
       " 'Information about the current running task, avaialble on the workers and experimental.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file = sc.textFile(textFile_path)\n",
    "text_file.filter(lambda line: line != '').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Public', 'classes:', 'SparkContext:', 'Main', 'entry']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatMap: split lines in text file; Useful for word count\n",
    "text_file.flatMap(lambda line: line.split(' ')).filter(lambda word: word!='').take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Transformations\n",
    "Transformation is a function that produces new RDD from the existing RDDs but when we want to work with the actual dataset, at that point Action is performed. When the action is triggered after the result, new RDD is not formed like transformation. Spark Transformation is a function hat produces new RDD from the existing RDDs. It takes RDD as input and produces one or more RDD as output. Each time it creates new RDD when we apply any transformation. Thus, the so input RDDs, cannot be changed since RDD are immutable in nature.\n",
    "\n",
    "Applying transformation built an RDD lineage, with the entire parent RDDs of the final RDD(s). RDD lineage, also known as RDD operator graph or RDD dependency graph. It is a logical execution plan i.e., it is Direct Acyclic Graph (DAG) or the entire parent RDDs of RDD.\n",
    "\n",
    "Transformations are lazy in nature ie.e, they get executed when we call an action. They are not executed immediately. Two most basic type of transformation is a map(), filter(). After the transformation, the resultant RDD is always different from its parent RDD. It can be smaller (e.g. filter, count, distinct, sample), bigger (e.g. flatMap(), union(), Cartesian()) or the same size (e.g. map)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map, filter, flatMap, distinct, sortBy, groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\"\"',\n",
       "  '\"id\"',\n",
       "  '\"track_name\"',\n",
       "  '\"size_bytes\"',\n",
       "  '\"currency\"',\n",
       "  '\"price\"',\n",
       "  '\"rating_count_tot\"',\n",
       "  '\"rating_count_ver\"',\n",
       "  '\"user_rating\"',\n",
       "  '\"user_rating_ver\"',\n",
       "  '\"ver\"',\n",
       "  '\"cont_rating\"',\n",
       "  '\"prime_genre\"',\n",
       "  '\"sup_devices.num\"',\n",
       "  '\"ipadSc_urls.num\"',\n",
       "  '\"lang.num\"',\n",
       "  '\"vpp_lic\"'],\n",
       " ['\"1\"',\n",
       "  '\"281656475\"',\n",
       "  '\"PAC-MAN Premium\"',\n",
       "  '100788224',\n",
       "  '\"USD\"',\n",
       "  '3.99',\n",
       "  '21292',\n",
       "  '26',\n",
       "  '4',\n",
       "  '4.5',\n",
       "  '\"6.3.5\"',\n",
       "  '\"4+\"',\n",
       "  '\"Games\"',\n",
       "  '38',\n",
       "  '5',\n",
       "  '10',\n",
       "  '1']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile(appleStore_path)\n",
    "# map: replace comma in string and create a list for each record\n",
    "lines = rdd.map(lambda line: line.replace(\", \", \" \")).map(lambda line: line.split(','))\n",
    "lines.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\"175\"', '\"289\"', '\"348\"', '\"418\"', '\"554\"']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of columns in each record\n",
    "lines_wrong_parsing = lines.filter(lambda line: len(line) > 17).map(lambda cols: cols[0])\n",
    "print(lines_wrong_parsing.count())\n",
    "lines_wrong_parsing.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"554\",\"385285922\",\"乐视视频-白鹿原,欢乐颂,奔跑吧全网热播\",184689664,\"USD\",0,1590,6,4.5,5,\"7.1\",\"17+\",\"Entertainment\",38,0,2,1']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda line: line.startswith('\"554\"')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7163"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter: getting rid of header and rows can't be parsed correctly\n",
    "lines = lines.filter(lambda cols: cols[0] != '\"\"').filter(lambda line: len(line) == 17)\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7163\n",
      "7163\n"
     ]
    }
   ],
   "source": [
    "# distinct: check whether there are duplicate records\n",
    "ids = lines.map(lambda cols: cols[0])\n",
    "print(ids.count())\n",
    "print(ids.distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Facebook\": 2974676',\n",
       " '\"Instagram\": 2161558',\n",
       " '\"Clash of Clans\": 2130805',\n",
       " '\"Temple Run\": 1724546',\n",
       " '\"Pandora - Music & Radio\": 1126879']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortBy: get the top 5 apps with the most total rating\n",
    "lines.sortBy(lambda line: int(line[6]), False).map(lambda cols: cols[2]+\": \"+cols[6]).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Games\": 3848',\n",
       " '\"Entertainment\": 531',\n",
       " '\"Education\": 453',\n",
       " '\"Photo & Video\": 346',\n",
       " '\"Utilities\": 247',\n",
       " '\"Health & Fitness\": 178',\n",
       " '\"Productivity\": 178',\n",
       " '\"Social Networking\": 165',\n",
       " '\"Lifestyle\": 141',\n",
       " '\"Music\": 138',\n",
       " '\"Shopping\": 121',\n",
       " '\"Sports\": 114',\n",
       " '\"Book\": 112',\n",
       " '\"Finance\": 103',\n",
       " '\"Travel\": 80',\n",
       " '\"News\": 75',\n",
       " '\"Weather\": 72',\n",
       " '\"Reference\": 64',\n",
       " '\"Food & Drink\": 62',\n",
       " '\"Business\": 56',\n",
       " '\"Navigation\": 46',\n",
       " '\"Medical\": 23',\n",
       " '\"Catalogs\": 10']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupBy: group the apps by genre - returns a literable object for each group\n",
    "grp = lines.groupBy(lambda cols: cols[12])\n",
    "grp_sorted = grp.sortBy(lambda group: len(list(group[1])), False)\n",
    "grp_count = grp_sorted.map(lambda group: group[0] + \": \" + str(len(list(group[1]))))\n",
    "grp_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\"Food & Drink\"', 62), ('\"Games\"', 3848), ('\"Shopping\"', 121), ('\"Business\"', 56), ('\"Health & Fitness\"', 178)]\n"
     ]
    }
   ],
   "source": [
    "# key-value expression for grouped RDD\n",
    "print([(k, len(list(v))) for k, v in grp.take(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key-Value Pair RDD (a.k.a PairRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyBy, foldByKey, reduceByKey, groupByKey, lookup, mapValues, collectAsMap, countByKey, sortByKey, sampleByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('p', 'Public'),\n",
       " ('c', 'classes'),\n",
       " ('s', 'SparkContext'),\n",
       " ('m', 'Main'),\n",
       " ('e', 'entry'),\n",
       " ('p', 'point'),\n",
       " ('f', 'for'),\n",
       " ('s', 'Spark'),\n",
       " ('f', 'functionality.'),\n",
       " ('r', 'RDD')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = text_file.flatMap(lambda line: line.replace(':','').split(' ')).filter(lambda word: word != '')\n",
    "keywords = words.keyBy(lambda word: word.lower()[0])\n",
    "keywords.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('configuring', 1),\n",
       " ('cache', 1),\n",
       " ('only', 1),\n",
       " ('sparkcontext', 1),\n",
       " ('(rdd),', 1),\n",
       " ('variable', 2),\n",
       " ('public', 1),\n",
       " ('tasks', 2),\n",
       " ('files', 1),\n",
       " ('storagelevel', 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# foldByKey - require a value may be added to the result an arbitrary number of times,\n",
    "# and must not change the result (e.g. 0 for addition, or 1 for multiplication)\n",
    "word_counts = words.map(lambda word: (word.replace('.','').lower(), 1))\n",
    "word_counts.foldByKey(0, add).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('configuring', 1),\n",
       " ('cache', 1),\n",
       " ('only', 1),\n",
       " ('sparkcontext', 1),\n",
       " ('(rdd),', 1),\n",
       " ('variable', 2),\n",
       " ('public', 1),\n",
       " ('tasks', 2),\n",
       " ('files', 1),\n",
       " ('storagelevel', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey\n",
    "word_counts.reduceByKey(lambda a,b: a+b).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('g', 1), ('s', 9), ('“', 1), ('b', 3), ('l', 1), ('r', 4), ('c', 5), ('i', 2), ('p', 3), ('j', 1), ('d', 2), ('e', 2), ('v', 3), ('m', 1), ('o', 2), ('f', 5), ('a', 11), ('t', 10), ('w', 2), ('(', 1)]\n"
     ]
    }
   ],
   "source": [
    "# groupByKey\n",
    "keyword_freq = keywords.groupByKey()\n",
    "print(list((w[0], len(list(w[1]))) for w in keyword_freq.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: groupBy and groupByKey can cause out of memory exceptions and are expensive operations.\n",
    "\n",
    "While both groupByKey and reduceByKey can produce the correct answer, the reduceByKey example works much better on a large dataset. That's because Spark knows it can combine output with a common key on each partition before shuffling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SparkContext',\n",
       " 'Spark',\n",
       " 'Spark.',\n",
       " 'shared',\n",
       " 'SparkConf',\n",
       " 'Spark.',\n",
       " 'SparkFiles',\n",
       " 'shipped',\n",
       " 'StorageLevel']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lookup\n",
    "keywords.lookup('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('p', 'PUBLIC'),\n",
       " ('c', 'CLASSES'),\n",
       " ('s', 'SPARKCONTEXT'),\n",
       " ('m', 'MAIN'),\n",
       " ('e', 'ENTRY')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapValues\n",
    "keywords.mapValues(lambda word: word.upper()).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"1\"', '\"PAC-MAN Premium\"'),\n",
       " ('\"10\"', '\"Ms. PAC-MAN\"'),\n",
       " ('\"100\"', '\"Tempo - Metronome with Setlists\"'),\n",
       " ('\"1000\"', '\"PDF Converter - Convert Documents Photos to PDF\"'),\n",
       " ('\"10000\"', '\"Bus Simulator PRO 2017\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortByKey (using appstore data)\n",
    "pair = lines.map(lambda line: (line[0], line[2]))\n",
    "pair.sortByKey().take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'(': 1,\n",
       "             'a': 11,\n",
       "             'b': 3,\n",
       "             'c': 5,\n",
       "             'd': 2,\n",
       "             'e': 2,\n",
       "             'f': 5,\n",
       "             'g': 1,\n",
       "             'i': 2,\n",
       "             'j': 1,\n",
       "             'l': 1,\n",
       "             'm': 1,\n",
       "             'o': 2,\n",
       "             'p': 3,\n",
       "             'r': 4,\n",
       "             's': 9,\n",
       "             't': 10,\n",
       "             'v': 3,\n",
       "             'w': 2,\n",
       "             '“': 1})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countByKey\n",
    "keywords.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'b', 'h', 'i', '.', 'p', 'd', 'g', 'l', 'c', 'y', '“', 'j', 'r', ',', 't', 'e', 'k', 'x', '-', 'a', 'f', 'n', '(', 'm', '”', 'v', 'o', 'u', ')', 'w']\n",
      "{')': 0.008953684383868121, 'u': 0.026902177687455175, 'k': 0.3448540677574369, 'x': 0.7552635674996473, '-': 0.97775959373821, 'i': 0.26437577711493276, 'a': 0.5377169379396061, 'f': 0.5431578466842806, 'p': 0.4242352354851444, 'd': 0.7923890721797011, '(': 0.9435348472204684, 'g': 0.18427646027941935, 'l': 0.17174585638652773, '”': 0.5790377391313657, 'c': 0.38377829630176985, ',': 0.2806753902658189, 'y': 0.12262615340007577, 'e': 0.7460624175554434, 'w': 0.5724674011077882, 'n': 0.5668159712275453, 'b': 0.6321963742423565, 'h': 0.046067056892209135, 'm': 0.7828488731085762, '.': 0.4598671439552072, 't': 0.781857179104545, 's': 0.015724672011185326, 'r': 0.08701146150158123, 'o': 0.7358846420832192, 'v': 0.6711551498055514, '“': 0.8340043969601465, 'j': 0.11867723815618325}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('c', 'classes'),\n",
       " ('m', 'Main'),\n",
       " ('f', 'for'),\n",
       " ('f', 'functionality.'),\n",
       " ('d', 'Distributed'),\n",
       " ('t', 'the'),\n",
       " ('b', 'basic'),\n",
       " ('b', 'basic'),\n",
       " ('a', 'abstraction'),\n",
       " ('i', 'in'),\n",
       " ('i', 'in'),\n",
       " ('b', 'Broadcast'),\n",
       " ('b', 'Broadcast'),\n",
       " ('b', 'Broadcast'),\n",
       " ('b', 'broadcast'),\n",
       " ('v', 'variable'),\n",
       " ('t', 'that'),\n",
       " ('t', 'that'),\n",
       " ('t', 'tasks.'),\n",
       " ('a', 'An'),\n",
       " ('a', 'An'),\n",
       " ('“', '“add-only”'),\n",
       " ('v', 'variable'),\n",
       " ('v', 'variable'),\n",
       " ('o', 'only'),\n",
       " ('v', 'values'),\n",
       " ('t', 'to.'),\n",
       " ('f', 'files'),\n",
       " ('f', 'files'),\n",
       " ('w', 'with'),\n",
       " ('t', 'TaskContext'),\n",
       " ('t', 'task,'),\n",
       " ('o', 'on'),\n",
       " ('t', 'the')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampleByKey (deterministic(T/F), fraction, seed)\n",
    "distinctChars = words.flatMap(lambda word: list(word.lower())).distinct().collect()\n",
    "print(distinctChars)\n",
    "    # assign a random number to each character -> \n",
    "        # the percentage samples related to the key to be draw\n",
    "sampleMap = dict(map(lambda c: (c, random.random()), distinctChars))\n",
    "print(sampleMap)\n",
    "words.map(lambda word: (word.lower()[0], word)).sampleByKey(True, sampleMap, 6).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Functions - Math / Statistical\n",
    "These are functions (precisely actions) that are supported over RDD of doubles (in Java parlance). In PySpark, these are regular functions that expect list of int/float values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min, max, mean, sum, variance, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.0, 4.0, 3.5, 4.0, 4.5]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings = lines.map(lambda cols: float(cols[8]))\n",
    "user_ratings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.0\n",
      "Max: 5.0\n",
      "Sum: 25289.0\n",
      "Mean: 3.530503978779842\n",
      "Std: 1.5148105385\n",
      "Variance: 2.294650967560603\n"
     ]
    }
   ],
   "source": [
    "print(\"Min:\", user_ratings.min())\n",
    "print(\"Max:\", user_ratings.max())\n",
    "print(\"Sum:\", user_ratings.sum())\n",
    "print(\"Mean:\", user_ratings.mean())\n",
    "print(\"Std:\", user_ratings.stdev())\n",
    "print(\"Variance:\", user_ratings.variance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 7163, mean: 3.530503978779842, stdev: 1.5148105385, max: 5.0, min: 0.0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first, count, collect, take, top, reduce, takeOrdered, countByValue, countByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2974676, 2161558, 2130805]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_rating_count = lines.map(lambda cols: int(cols[6]))\n",
    "# expensive when the data is big\n",
    "total_rating_count.top(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PySpark', 'is', 'the', 'Python', 'API', 'for', 'Spark.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcoll.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'is', 'for']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcoll.top(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PySpark'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcoll.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "rdd_num = sc.parallelize([1,2,3,4,5])\n",
    "print(rdd_num.reduce(add))\n",
    "print(rdd_num.reduce(mul))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expensive when the data is big\n",
    "total_rating_count.takeOrdered(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('cache', 1), ('Main', 1), ('Dataset', 1), ('Information', 1), ('Accumulator', 1), ('that', 2), ('for', 1), ('shipped', 1), ('Spark.', 2), ('about', 1), ('the', 3), ('classes', 1), ('point', 1), ('workers', 1), ('experimental.', 1), ('avaialble', 1), ('persistence', 1), ('functionality.', 1), ('entry', 1), ('tasks.', 1), ('For', 1), ('gets', 1), ('current', 1), ('Finer-grained', 1), ('basic', 1), ('running', 1), ('SparkContext', 1), ('jobs.', 1), ('StorageLevel', 1), ('“add-only”', 1), ('A', 2), ('configuring', 1), ('only', 1), ('shared', 1), ('variable', 2), ('across', 1), ('RDD', 1), ('task,', 1), ('files', 1), ('in', 1), ('values', 1), ('Public', 1), ('Access', 1), ('add', 1), ('Broadcast', 1), ('reused', 1), ('on', 1), ('SparkFiles', 1), ('with', 1), ('and', 1), ('Spark', 1), ('tasks', 1), ('abstraction', 1), ('SparkConf', 1), ('An', 1), ('can', 1), ('Distributed', 1), ('Resilient', 1), ('(RDD),', 1), ('TaskContext', 1), ('to.', 1), ('broadcast', 1), ('levels.', 1)])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.countByValue().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('s', 9), ('b', 3), ('m', 1), ('o', 2), ('r', 4), ('a', 11), ('j', 1), ('t', 10), ('p', 3), ('d', 2), ('(', 1), ('g', 1), ('i', 2), ('f', 5), ('c', 5), ('v', 3), ('l', 1), ('e', 2), ('“', 1), ('w', 2)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords.countByKey().items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Partitions & Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(25), 3)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared Variables\n",
    "- Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6, 9, 12]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = sc.broadcast([3,6,9,12])\n",
    "b.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6, 9, 12, 3, 6, 9, 12]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([0,0]).flatMap(lambda x: b.value).collect() # each 0 will be replaced by broadcasted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6, 9, 12, 3, 6, 9, 12]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unpersist() # using unpersist only removes broadcast variable from executor\n",
    "sc.parallelize([0,0]).flatMap(lambda x: b.value).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove it from driver as well use\n",
    "b.destroy()\n",
    "#sc.parallelize([0,0]).flatMap(lambda x: b.value).collect() # results in error as boradcast variable is destroyed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=0, value=0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.accumulator(0)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([3,4,5]).foreach(lambda x: a.add(x))\n",
    "a.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resetting an accumulator\n",
    "a.value = 0\n",
    "a.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.value += 3\n",
    "a.value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
